"""
Kafka Consumer Service for Data Storage - FULLY INSTRUMENTED VERSION
Consumes streaming data from Kafka topics and stores to PostgreSQL with extensive logging

This service bridges the gap between Kafka streaming and persistent storage:
- Consumes from wildfire-nasa-firms, wildfire-weather-data, wildfire-sensor-data topics
- Transforms and validates data according to PostgreSQL schema
- Stores fire incidents, weather data, and sensor readings
- Handles retries and error recovery
- FULLY INSTRUMENTED FOR DEBUGGING MESSAGE PROCESSING FAILURES


This script is essentially a **fully-instrumented Kafka consumer service** designed to reliably consume wildfire-related streaming data and store it in a PostgreSQL database. Here's a detailed explanation:

---

## **1. Purpose**

The service acts as a bridge between **Kafka streaming topics** and **persistent storage**:

* Consumes messages from topics like:

  * `wildfire-nasa-firms` (fire detections)
  * `wildfire-weather-data` (weather readings)
  * `wildfire-sensor-data` (IoT sensor readings)
  * Satellite or imagery topics (`wildfire-satellite-data`, `wildfire-satellite-imagery`)
* Validates and transforms messages to match PostgreSQL schemas.
* Stores processed data in corresponding database tables.
* Fully instrumented for debugging, with detailed logging at every step.

---

## **2. Core Classes**

### **2.1 KafkaDataConsumer**

The main class responsible for consuming and processing Kafka messages.

**Key components:**

* **Initialization (`__init__`)**

  * Accepts a `DatabaseManager` instance for DB operations.
  * Sets Kafka topics and maps them to their respective handlers (`_handle_fire_detection`, `_handle_weather_data`, `_handle_sensor_data`, etc.).
  * Configurable `auto_offset_reset` (`earliest` by default) controls where the consumer starts reading in Kafka if offsets are missing.

* **Starting (`start`)**

  * Connects to Kafka with **retry logic** (max 5 attempts with exponential backoff).
  * Launches `_consume_messages()` loop after connection.

* **Stopping (`stop`)**

  * Gracefully stops the consumer.

* **Message Consumption (`_consume_messages`)**

  * Loops over Kafka messages asynchronously.
  * Logs every message extensively (topic, partition, offset, key, timestamp, size, snippet of data).
  * Tries to parse JSON payloads, handles decoding errors.
  * Routes messages to the appropriate handler based on topic.
  * Tracks statistics: total messages, successful, failed, and logs periodically.

---

## **3. Data Handlers**

Each topic has a handler method that validates, transforms, and stores data.

### **3.1 Fire Detection (`_handle_fire_detection`)**

* Handles NASA FIRMS fire detection messages.
* Processes nested structures in JSON.
* Stores each fire incident in `fire_incidents` table.
* Uses `_store_fire_incident()` for PostgreSQL insertion.
* Robust timestamp parsing (`_parse_sophisticated_timestamp`) from `acq_date` + `acq_time` fields.

### **3.2 Weather Data (`_handle_weather_data`)**

* Handles NOAA/weather messages.
* Extracts relevant fields like temperature, humidity, wind, pressure, timestamp.
* Stores each record in `weather_data` table.
* Uses `_store_weather_record()` with dynamic insert based on table schema.

### **3.3 Sensor Data (`_handle_sensor_data`)**

* Handles IoT sensor messages.
* Extracts readings, processes timestamp, and stores in `sensor_readings` table.
* `_store_sensor_record()` ensures only mapped/nullable columns are inserted.

### **3.4 Satellite Data (`_handle_satellite_data` / `_handle_satellite_imagery`)**

* Currently logs messages without DB storage.
* Can be extended to store satellite imagery or general satellite data in future tables.

---

## **4. Timestamp Parsing**

* `_parse_timestamp()` and `_parse_sophisticated_timestamp()` handle multiple formats:

  * ISO timestamps (`2025-09-22T12:00:00Z`)
  * NASA FIRMS format (`acq_date` + `acq_time`)
  * Fallback to current UTC time if parsing fails

---

## **5. Database Interaction**

* Uses `DatabaseManager` for PostgreSQL connections.
* Dynamically queries table schemas (`information_schema.columns`) to ensure proper insertion.
* Handles:

  * Required columns (`id`, `reading_id`, `incident_id`)
  * Nullable columns
  * Default values for missing data (e.g., `wind_direction=0.0`, `pressure=1013.25`)
* Each insertion logs every detail:

  * Values, types, query, and success/failure.

---

## **6. KafkaConsumerManager**

* A **wrapper class** for lifecycle management:

  * Starts/stops the consumer asynchronously.
  * Maintains background task (`asyncio.create_task`) for the consumer loop.
  * Provides `is_running()` method to check status.

---

## **7. Logging / Instrumentation**

* Uses `structlog` for structured logging.
* Logs every step of message processing:

  * Reception
  * JSON parsing
  * Routing to handler
  * DB insertion
  * Errors and retries
* Includes previews of raw data for debugging.
* Maintains detailed statistics for monitoring message processing rates.

---

## **8. Summary Flow**

1. `KafkaConsumerManager.start()` ‚Üí creates a `KafkaDataConsumer` instance.
2. `_consume_messages()` asynchronously iterates over Kafka messages.
3. For each message:

   * Parse JSON
   * Route to handler
   * Transform & validate data
   * Store in PostgreSQL
   * Log successes/failures
4. `_store_*` methods dynamically insert into DB based on schema.
5. Supports stopping the consumer gracefully via `stop()`.

"""

import asyncio
import json
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional
import structlog

from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaError
from ..models.database import DatabaseManager
from ..config import get_settings
from ..utils.table_router import TableRouter

logger = structlog.get_logger()

class KafkaDataConsumer:
    """
    Kafka Consumer for streaming data storage with comprehensive instrumentation

    Consumes messages from Kafka topics and stores to PostgreSQL
    with proper data transformation and error handling.
    """

    def __init__(self, db_manager: DatabaseManager, auto_offset_reset: str = None):
        self.db_manager = db_manager
        self.settings = get_settings()
        self.consumer = None
        self.running = False
        self.auto_offset_reset = auto_offset_reset or self.settings.kafka_auto_offset_reset

        # Topic to database table mapping - updated to match exact topic patterns
        self.topic_handlers = {
            'wildfire-nasa-firms': self._handle_fire_detection,
            'wildfire-weather-data': self._handle_weather_data,  # Real-time weather (stations, observations)
            'wildfire-weather-alerts': self._handle_weather_data,  # Time-critical alerts (parallel processing)
            'wildfire-weather-bulk': self._handle_weather_data,  # Bulk weather (ERA5, GFS, NAM - parallel processing)
            'wildfire-sensor-data': self._handle_sensor_data,
            'wildfire-iot-sensors': self._handle_sensor_data,  # IoT sensor data
            'wildfire-satellite-data': self._handle_satellite_data,  # General satellite data
            'wildfire-satellite-imagery': self._handle_satellite_imagery,  # Satellite imagery
            'wildfire-incidents': self._handle_fire_detection,  # Fire incidents
            # 'wildfire-weather-gfs': self._handle_weather_data,  # Disabled: old topic with snappy messages
        }

    async def start(self):
        """Start Kafka consumer with retry logic"""
        import asyncio

        max_retries = 5
        retry_delay = 5

        for attempt in range(max_retries):
            try:
                logger.info(f"üîÑ KAFKA CONSUMER START ATTEMPT {attempt + 1}/{max_retries}")

                # Initialize Kafka consumer
                self.consumer = AIOKafkaConsumer(
                    *self.topic_handlers.keys(),
                    bootstrap_servers=self.settings.kafka_bootstrap_servers,
                    group_id=self.settings.kafka_consumer_group_id,
                    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                    auto_offset_reset=self.auto_offset_reset,  # Use configurable offset reset
                    enable_auto_commit=True,
                    auto_commit_interval_ms=5000
                )

                logger.info(f"üåü CONNECTING TO KAFKA: {self.settings.kafka_bootstrap_servers}")
                await self.consumer.start()
                self.running = True
                logger.info("‚úÖ KAFKA CONSUMER STARTED SUCCESSFULLY")
                break

            except Exception as e:
                logger.error(f"‚ùå KAFKA CONSUMER START ATTEMPT {attempt + 1} FAILED",
                           error=str(e),
                           error_type=type(e).__name__)

                if attempt < max_retries - 1:
                    logger.info(f"‚è≥ WAITING {retry_delay}s BEFORE RETRY {attempt + 2}/{max_retries}")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    logger.error("üí• KAFKA CONSUMER FAILED TO START AFTER ALL RETRIES")
                    raise

        logger.info("Kafka consumer started",
                   topics=list(self.topic_handlers.keys()),
                   bootstrap_servers=self.settings.kafka_bootstrap_servers)

        # Start consuming messages
        await self._consume_messages()

    async def stop(self):
        """Stop Kafka consumer"""
        self.running = False
        if self.consumer:
            await self.consumer.stop()
            logger.info("Kafka consumer stopped")

    async def _consume_messages(self):
        """Main message consumption loop with detailed instrumentation"""
        logger.info("üöÄ STARTING MESSAGE CONSUMPTION LOOP WITH FULL INSTRUMENTATION",
                   consumer_group_id='data-storage-consumer',
                   topics=list(self.topic_handlers.keys()),
                   auto_offset_reset='latest')

        message_count = 0
        successful_messages = 0
        failed_messages = 0

        try:
            logger.info("üì• ENTERING KAFKA MESSAGE ITERATOR LOOP")

            async for message in self.consumer:
                message_count += 1

                logger.info(f"üì® RECEIVED KAFKA MESSAGE #{message_count}",
                           topic=message.topic,
                           partition=message.partition,
                           offset=message.offset,
                           key=str(message.key) if message.key else None,
                           timestamp=message.timestamp,
                           timestamp_type=message.timestamp_type)

                if not self.running:
                    logger.warning("üõë CONSUMER STOPPING - BREAKING FROM LOOP",
                                 messages_processed=message_count)
                    break

                topic = message.topic
                raw_data = message.value

                logger.info(f"üîç PROCESSING MESSAGE #{message_count}",
                           topic=topic,
                           raw_data_type=type(raw_data).__name__,
                           raw_data_size=len(str(raw_data)),
                           raw_data_preview=str(raw_data)[:500] if raw_data else "None")

                # Parse JSON message data with detailed logging
                try:
                    logger.info(f"üîß JSON PARSING MESSAGE #{message_count}",
                              original_type=type(raw_data).__name__)

                    if isinstance(raw_data, bytes):
                        logger.info(f"‚öôÔ∏è DECODING BYTES TO UTF-8 FOR MESSAGE #{message_count}")
                        raw_data = raw_data.decode('utf-8')

                    if isinstance(raw_data, str):
                        logger.info(f"‚öôÔ∏è PARSING JSON STRING FOR MESSAGE #{message_count}",
                                  json_string_preview=raw_data[:200])
                        data = json.loads(raw_data)
                    else:
                        logger.info(f"‚öôÔ∏è USING RAW DATA AS-IS FOR MESSAGE #{message_count}")
                        data = raw_data

                    logger.info(f"‚úÖ JSON PARSING SUCCESS FOR MESSAGE #{message_count}",
                              parsed_data_type=type(data).__name__,
                              parsed_keys=list(data.keys()) if isinstance(data, dict) else "not_dict")

                except json.JSONDecodeError as e:
                    failed_messages += 1
                    logger.error(f"‚ùå JSON PARSING FAILED FOR MESSAGE #{message_count}",
                               topic=topic,
                               error=str(e),
                               error_type=type(e).__name__,
                               raw_data_preview=str(raw_data)[:500])
                    continue
                except Exception as e:
                    failed_messages += 1
                    logger.error(f"‚ùå UNEXPECTED JSON PARSING ERROR FOR MESSAGE #{message_count}",
                               topic=topic,
                               error=str(e),
                               error_type=type(e).__name__)
                    continue

                # Route message to appropriate handler with detailed logging
                if topic in self.topic_handlers:
                    logger.info(f"üéØ ROUTING MESSAGE #{message_count} TO HANDLER",
                              topic=topic,
                              handler_function=self.topic_handlers[topic].__name__)
                    try:
                        logger.info(f"üöÄ CALLING HANDLER FOR MESSAGE #{message_count}",
                                   topic=topic,
                                   data_preview=str(data)[:300] if data else "None")

                        await self.topic_handlers[topic](data)

                        successful_messages += 1
                        logger.info(f"‚úÖ MESSAGE #{message_count} PROCESSED SUCCESSFULLY",
                                   topic=topic,
                                   success_rate=f"{successful_messages}/{message_count}")

                    except Exception as e:
                        failed_messages += 1
                        logger.error(f"‚ùå HANDLER FAILED FOR MESSAGE #{message_count}",
                                   topic=topic,
                                   error=str(e),
                                   error_type=type(e).__name__,
                                   message_data_preview=str(data)[:500] if data else "None",
                                   failure_rate=f"{failed_messages}/{message_count}")
                        # Continue processing other messages despite handler failure
                else:
                    failed_messages += 1
                    logger.warning(f"‚ö†Ô∏è NO HANDLER FOUND FOR MESSAGE #{message_count}",
                                 topic=topic,
                                 available_handlers=list(self.topic_handlers.keys()))

                # Log periodic statistics
                if message_count % 10 == 0:
                    logger.info(f"üìä MESSAGE PROCESSING STATISTICS",
                              total_messages=message_count,
                              successful_messages=successful_messages,
                              failed_messages=failed_messages,
                              success_rate=f"{(successful_messages/message_count)*100:.1f}%" if message_count > 0 else "0%")

        except KafkaError as e:
            logger.error("üí• KAFKA ERROR DURING CONSUMPTION",
                        error=str(e),
                        error_type=type(e).__name__,
                        messages_processed=message_count,
                        successful_messages=successful_messages,
                        failed_messages=failed_messages)
        except Exception as e:
            logger.error("üí• UNEXPECTED ERROR DURING MESSAGE CONSUMPTION",
                        error=str(e),
                        error_type=type(e).__name__,
                        messages_processed=message_count,
                        successful_messages=successful_messages,
                        failed_messages=failed_messages,
                        exc_info=True)
        finally:
            logger.info("üèÅ MESSAGE CONSUMPTION LOOP ENDED",
                       total_messages_processed=message_count,
                       successful_messages=successful_messages,
                       failed_messages=failed_messages,
                       final_success_rate=f"{(successful_messages/message_count)*100:.1f}%" if message_count > 0 else "0%")

    async def _handle_fire_detection(self, data: Dict[str, Any]):
        """
        Handle fire detection data from NASA FIRMS

        Actual Kafka message format (flat structure with individual detections):
        {
            "timestamp": "...",
            "latitude": 40.822,
            "longitude": -114.256,
            "confidence": 0.5,
            "bright_ti4": 301.2,
            "frp": 1.5,
            "satellite": "Terra",
            "instrument": "MODIS",
            "ingestion_metadata": {
                "source_id": "firms_modis_terra",
                "source_type": "satellite"
            }
        }
        """
        try:
            # Extract source_id from ingestion_metadata (flat message format)
            ingestion_metadata = data.get('ingestion_metadata', {})
            source_id = ingestion_metadata.get('source_id', data.get('source', 'unknown'))

            if source_id == 'unknown':
                logger.error("Fire detection source_id unknown - cannot route to table",
                           available_keys=list(data.keys()),
                           has_ingestion_metadata=bool(ingestion_metadata))
                return

            # Use TableRouter to determine target table
            table_name = TableRouter.get_table_name(source_id)

            logger.debug(f"Routing fire detection to {table_name}",
                       source_id=source_id,
                       latitude=data.get('latitude'),
                       longitude=data.get('longitude'))

            # Map fields and store to specialized table
            mapped_data = TableRouter.map_fields(source_id, data)
            await self._store_to_table(table_name, mapped_data, source_id)

            logger.info("Stored fire detection from Kafka",
                       source_id=source_id,
                       table=table_name)

        except Exception as e:
            logger.error("Failed to handle fire detection data",
                       error=str(e),
                       source_id=source_id if 'source_id' in locals() else 'unknown')
            raise

    async def _store_fire_incident(self, detection: Dict[str, Any], source_id: str):
        """Store individual fire incident to PostgreSQL with detailed logging"""
        incident_id = str(uuid.uuid4())

        logger.info("üî• STORING FIRE INCIDENT TO DATABASE",
                   incident_id=incident_id,
                   source_id=source_id,
                   detection_keys=list(detection.keys()) if detection else [],
                   latitude=detection.get('latitude'),
                   longitude=detection.get('longitude'))

        try:
            # Transform detection data to match PostgreSQL schema
            logger.info("‚öôÔ∏è TRANSFORMING FIRE DETECTION DATA",
                       incident_id=incident_id,
                       original_detection=detection)

            # Handle both main system format (bright_ti4, bright_ti5) and simplified format
            brightness = detection.get('brightness') or detection.get('bright_ti4')
            temperature = detection.get('temperature') or detection.get('bright_ti5') or brightness
            parsed_timestamp = self._parse_sophisticated_timestamp(detection)

            incident_data = {
                'incident_id': incident_id,
                'latitude': float(detection.get('latitude', 0.0)),
                'longitude': float(detection.get('longitude', 0.0)),
                'confidence': float(detection.get('confidence', 0.0)),
                'brightness': brightness,
                'temperature': temperature,
                'frp': detection.get('frp'),
                'bright_t31': detection.get('bright_t31'),
                'daynight': detection.get('daynight', 'D'),
                'satellite': detection.get('satellite'),
                'instrument': detection.get('instrument'),
                'source': source_id,
                'timestamp': parsed_timestamp,
                'created_at': datetime.utcnow(),
                'type': 0  # Default fire type
            }

            logger.info("üìù PREPARED FIRE INCIDENT DATA FOR DB INSERT",
                       incident_id=incident_id,
                       prepared_data={
                           k: str(v) if v is not None else None
                           for k, v in incident_data.items()
                       })

            # Insert into PostgreSQL with detailed logging
            logger.info("üîó OBTAINING DATABASE CONNECTION FOR FIRE INCIDENT",
                       incident_id=incident_id)

            async with self.db_manager.get_connection() as conn:
                logger.info("‚úÖ DATABASE CONNECTION OBTAINED FOR FIRE INCIDENT",
                           incident_id=incident_id,
                           connection_info=str(conn)[:100])

                query = """
                INSERT INTO fire_incidents (
                    incident_id, latitude, longitude, confidence, brightness,
                    temperature, frp, bright_t31, daynight, satellite,
                    instrument, source, timestamp, created_at, type
                ) VALUES (
                    $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15
                )
                """

                logger.info("üöÄ EXECUTING FIRE INCIDENT INSERT QUERY",
                           incident_id=incident_id,
                           query=query,
                           values_count=len(incident_data),
                           value_types=[type(v).__name__ for v in incident_data.values()])

                try:
                    result = await conn.execute(query, *incident_data.values())

                    logger.info("‚úÖ FIRE INCIDENT SUCCESSFULLY INSERTED TO DATABASE",
                               incident_id=incident_id,
                               source_id=source_id,
                               db_result=str(result),
                               latitude=incident_data['latitude'],
                               longitude=incident_data['longitude'],
                               timestamp=str(incident_data['timestamp']))

                except Exception as db_error:
                    logger.error("‚ùå DATABASE INSERT FAILED FOR FIRE INCIDENT",
                                incident_id=incident_id,
                                source_id=source_id,
                                db_error=str(db_error),
                                db_error_type=type(db_error).__name__,
                                query=query,
                                values=str(incident_data),
                                exc_info=True)
                    raise

        except Exception as e:
            logger.error("‚ùå FIRE INCIDENT STORAGE FAILED",
                        incident_id=incident_id,
                        source_id=source_id,
                        detection=detection,
                        error=str(e),
                        error_type=type(e).__name__,
                        exc_info=True)
            raise

    async def _store_to_table(self, table_name: str, data: Dict[str, Any], source_id: str):
        """
        Universal method to store data to any specialized table

        Uses dynamic schema introspection to build INSERT queries
        Works with all 24 specialized tables

        Args:
            table_name: Target PostgreSQL table name
            data: Data dictionary with field values
            source_id: Original source identifier for logging
        """
        record_id = str(uuid.uuid4())

        logger.info(f"üìä STORING TO {table_name.upper()}",
                   record_id=record_id,
                   source_id=source_id,
                   data_keys=list(data.keys()) if data else [])

        try:
            async with self.db_manager.get_connection() as conn:
                # Query table schema dynamically
                schema_query = """
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns
                WHERE table_name = $1
                AND table_schema = 'public'
                ORDER BY ordinal_position
                """

                schema_rows = await conn.fetch(schema_query, table_name)
                if not schema_rows:
                    logger.error(f"‚ùå TABLE NOT FOUND: {table_name}",
                               record_id=record_id,
                               source_id=source_id)
                    return

                logger.info(f"‚úÖ TABLE SCHEMA RETRIEVED: {table_name}",
                           record_id=record_id,
                           column_count=len(schema_rows))

                # Build dynamic INSERT based on schema and data
                available_columns = []
                values = []

                for row in schema_rows:
                    col_name = row['column_name']
                    col_type = row['data_type']

                    # Skip auto-generated columns
                    if col_name in ['id', 'created_at', 'updated_at']:
                        continue

                    # Get value from data
                    value = data.get(col_name)

                    if value is not None:
                        # Convert timestamp strings to datetime objects for TIMESTAMPTZ columns
                        if col_type in ['timestamp with time zone', 'timestamptz'] and isinstance(value, str):
                            try:
                                from dateutil import parser
                                value = parser.parse(value)
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Failed to parse timestamp for {col_name}: {value}",
                                             error=str(e))
                                # Use current time as fallback
                                value = datetime.utcnow()
                        
                        # Convert dict/list to JSON string for JSONB columns
                        elif col_type == 'jsonb' and isinstance(value, (dict, list)):
                            import json
                            value = json.dumps(value)

                        available_columns.append(col_name)
                        values.append(value)
                        logger.debug(f"‚úÖ MAPPED {col_name}: {value}")

                if not available_columns:
                    logger.warning(f"‚ö†Ô∏è NO COLUMNS MAPPED FOR {table_name}",
                                 record_id=record_id,
                                 source_id=source_id,
                                 data_keys=list(data.keys()))
                    return

                # Create INSERT query with placeholders
                placeholders = [f'${i}' for i in range(1, len(values) + 1)]

                # Add ON CONFLICT handling for tables with unique constraints
                if table_name == 'noaa_weather_alerts':
                    # Handle duplicate alert_id by updating the record
                    update_cols = [col for col in available_columns if col != 'alert_id']
                    if update_cols:
                        query = f"""
                        INSERT INTO {table_name} ({', '.join(available_columns)})
                        VALUES ({', '.join(placeholders)})
                        ON CONFLICT (alert_id) DO UPDATE SET
                            {', '.join([f"{col} = EXCLUDED.{col}" for col in update_cols])}
                        """
                    else:
                        # If only alert_id is being inserted, skip duplicates
                        query = f"""
                        INSERT INTO {table_name} ({', '.join(available_columns)})
                        VALUES ({', '.join(placeholders)})
                        ON CONFLICT (alert_id) DO NOTHING
                        """
                else:
                    query = f"""
                    INSERT INTO {table_name} ({', '.join(available_columns)})
                    VALUES ({', '.join(placeholders)})
                    """

                logger.info(f"üöÄ EXECUTING INSERT TO {table_name}",
                           record_id=record_id,
                           columns=available_columns,
                           column_count=len(available_columns))

                try:
                    result = await conn.execute(query, *values)
                    logger.info(f"‚úÖ SUCCESSFULLY INSERTED TO {table_name}",
                               record_id=record_id,
                               source_id=source_id,
                               db_result=str(result))

                except Exception as db_error:
                    logger.error(f"‚ùå INSERT FAILED TO {table_name}",
                                record_id=record_id,
                                source_id=source_id,
                                db_error=str(db_error),
                                query=query[:200],
                                exc_info=True)
                    raise

        except Exception as e:
            logger.error(f"‚ùå FAILED TO STORE TO {table_name}",
                        record_id=record_id,
                        source_id=source_id,
                        error=str(e),
                        exc_info=True)
            raise

    async def _handle_weather_data(self, data: Dict[str, Any]):
        """
        Handle weather data from NOAA/weather sources

        Expected data format:
        {
            "source_id": "noaa_gfs",
            "data": [
                {
                    "latitude": 40.0,
                    "longitude": -120.0,
                    "temperature": 25.5,
                    "humidity": 45.2,
                    "wind_speed": 10.5,
                    "wind_direction": 225,
                    "pressure": 1013.25,
                    "timestamp": "2025-09-22T12:00:00Z"
                }
            ]
        }
        """
        try:
            logger.info("üå°Ô∏è PROCESSING WEATHER MESSAGE",
                       message_keys=list(data.keys()) if data else [],
                       message_preview=str(data)[:200])

            # Extract source_id from metadata or use source field
            metadata = data.get('metadata', {})
            if not metadata:
                logger.warning("üìä DEBUG: No metadata found in weather message, using fallback empty dict",
                             available_keys=list(data.keys()) if data else [],
                             algorithm_impact="MEDIUM - Weather metadata missing")

            ingestion_metadata = data.get('ingestion_metadata', {})
            if not ingestion_metadata:
                logger.debug("üìä DEBUG: No ingestion_metadata found in weather message, using fallback empty dict",
                           available_keys=list(data.keys()) if data else [],
                           algorithm_impact="LOW - Ingestion metadata missing")

            source_id = metadata.get('source_id',
                       ingestion_metadata.get('source_id',
                       data.get('source_id',
                       data.get('source', 'unknown'))))
            if source_id == 'unknown':
                logger.error("üö® DEBUG: Weather source ID fallback to 'unknown' - CRITICAL for routing",
                           metadata_keys=list(metadata.keys()) if metadata else [],
                           ingestion_metadata_keys=list(ingestion_metadata.keys()) if ingestion_metadata else [],
                           data_keys=list(data.keys()) if data else [],
                           has_metadata_source_id=bool(metadata.get('source_id')),
                           has_ingestion_source_id=bool(ingestion_metadata.get('source_id')),
                           has_source=bool(data.get('source')),
                           algorithm_impact="HIGH - Weather data may be misclassified")

            # Try to get weather data from 'data' field first, then fallback to root level
            data_section = data.get('data', {})
            if not data_section:
                logger.warning("üìä DEBUG: No data section found in weather message, using fallback empty dict",
                             available_keys=list(data.keys()) if data else [],
                             algorithm_impact="HIGH - Weather data section missing")

            logger.info("üîç DEBUG: About to check data_section routing",
                       data_section_empty=not data_section,
                       data_section_type=type(data_section).__name__,
                       source_id=source_id)

            if not data_section:
                # If no 'data' field, check if weather data is at root level
                logger.info("üîç DEBUG: Checking message type",
                           source_id=source_id,
                           has_lat=bool(data.get('latitude')),
                           has_lon=bool(data.get('longitude')),
                           has_alert_id=bool(data.get('alert_id')),
                           alerts_in_source_id='alerts' in source_id if source_id else False)

                # Check source_id FIRST for alerts (may have dummy lat/lon)
                if source_id and "alerts" in source_id:
                    data_section = data
                    logger.info("üì¢ ALERT ROUTED BY SOURCE_ID", source_id=source_id)
                elif data.get('latitude') is not None and data.get('longitude') is not None:
                    data_section = data
                    logger.info("üìç WEATHER DATA FOUND AT ROOT LEVEL", source_id=source_id)
                elif 'alerts' in source_id and 'alert_id' in data:
                    # Weather alerts don't have lat/lon, they have alert_id and area_description
                    data_section = data
                    logger.info("üì¢ WEATHER ALERT FOUND AT ROOT LEVEL", source_id=source_id, alert_id=data.get('alert_id'))
                else:
                    logger.warning("‚ùå NO WEATHER DATA FOUND IN MESSAGE",
                                 source_id=source_id,
                                 has_data_field=bool(data.get('data')),
                                 has_lat_lon=bool(data.get('latitude')),
                                 has_alert_id=bool(data.get('alert_id')))
                    return

            # Store weather record - route to appropriate table based on source
            logger.info("üå§Ô∏è CALLING _store_weather_record",
                       data_keys=list(data_section.keys()) if data_section else [],
                       source_id=source_id)

            # Use TableRouter for automatic routing to specialized tables
            table_name = TableRouter.get_table_name(source_id)
            mapped_data = TableRouter.map_fields(source_id, data_section)
            # Fix alert-specific data before storing
            if source_id == 'noaa_alerts_active' and table_name == 'noaa_weather_alerts':
                import hashlib
                if not mapped_data.get('alert_id') or mapped_data.get('alert_id') == '':
                    unique_str = f"{mapped_data.get('event', 'unknown')}_{mapped_data.get('timestamp', '')}_{mapped_data.get('area_description', '')}"
                    mapped_data['alert_id'] = hashlib.md5(unique_str.encode()).hexdigest()
                    logger.info("Generated alert_id", alert_id=mapped_data['alert_id'])
                if 'sender' in mapped_data and 'sender_name' not in mapped_data:
                    mapped_data['sender_name'] = mapped_data.pop('sender')

            await self._store_to_table(table_name, mapped_data, source_id)

            logger.info("Stored weather data from Kafka",
                       source_id=source_id,
                       count=1)

        except Exception as e:
            logger.error("Failed to handle weather data", error=str(e))
            raise

    async def _store_weather_record(self, record: Dict[str, Any], source_id: str):
        """Store individual weather record to PostgreSQL with detailed logging"""
        reading_id = str(uuid.uuid4())

        logger.info("üå§Ô∏è STORING WEATHER RECORD TO DATABASE",
                   reading_id=reading_id,
                   source_id=source_id,
                   record_keys=list(record.keys()) if record else [],
                   record_preview=str(record)[:300])

        try:
            logger.info("üîó OBTAINING DATABASE CONNECTION FOR WEATHER RECORD",
                       reading_id=reading_id)

            async with self.db_manager.get_connection() as conn:
                logger.info("‚úÖ DATABASE CONNECTION OBTAINED FOR WEATHER RECORD",
                           reading_id=reading_id)

                # Query the table schema to get column names and types
                logger.info("üîç QUERYING WEATHER_DATA TABLE SCHEMA",
                           reading_id=reading_id)

                schema_query = """
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns
                WHERE table_name = 'weather_data'
                AND table_schema = 'public'
                ORDER BY ordinal_position
                """

                try:
                    schema_rows = await conn.fetch(schema_query)
                    logger.info("‚úÖ WEATHER TABLE SCHEMA RETRIEVED",
                               reading_id=reading_id,
                               column_count=len(schema_rows),
                               columns=[row['column_name'] for row in schema_rows])
                except Exception as schema_error:
                    logger.error("‚ùå FAILED TO RETRIEVE WEATHER TABLE SCHEMA",
                                reading_id=reading_id,
                                schema_error=str(schema_error),
                                schema_error_type=type(schema_error).__name__)
                    raise

                # Build dynamic insert based on available data and schema
                available_columns = []
                values = []

                logger.info("‚öôÔ∏è BUILDING DYNAMIC INSERT FOR WEATHER RECORD",
                           reading_id=reading_id)

                for row in schema_rows:
                    col_name = row['column_name']

                    logger.debug(f"üìù PROCESSING COLUMN: {col_name}",
                               reading_id=reading_id,
                               data_type=row['data_type'],
                               is_nullable=row['is_nullable'])

                    # Skip auto-generated columns (but include id since it's required)
                    if col_name in ['created_at']:
                        logger.debug(f"‚è≠Ô∏è SKIPPING AUTO-GENERATED COLUMN: {col_name}",
                                   reading_id=reading_id)
                        continue

                    # Handle required ID column
                    if col_name == 'id':
                        available_columns.append(col_name)
                        values.append(reading_id)
                        logger.debug(f"‚úÖ MAPPED id: {reading_id}")
                        continue

                    # Map data fields to database columns
                    if col_name == 'reading_id':
                        available_columns.append(col_name)
                        values.append(reading_id)
                        logger.debug(f"‚úÖ MAPPED reading_id: {reading_id}")

                    elif col_name == 'timestamp':
                        available_columns.append(col_name)

                        # Handle timestamp parsing with detailed logging
                        timestamp_str = record.get('timestamp')
                        logger.info("‚è∞ PROCESSING TIMESTAMP FOR WEATHER RECORD",
                                   reading_id=reading_id,
                                   original_timestamp=timestamp_str,
                                   timestamp_type=type(timestamp_str).__name__)

                        if timestamp_str:
                            try:
                                if timestamp_str.endswith('Z'):
                                    timestamp_str = timestamp_str[:-1] + '+00:00'
                                dt = datetime.fromisoformat(timestamp_str)
                                parsed_timestamp = dt.replace(tzinfo=None)
                                logger.info("‚úÖ TIMESTAMP PARSED SUCCESSFULLY",
                                           reading_id=reading_id,
                                           original=record.get('timestamp'),
                                           parsed=str(parsed_timestamp))
                            except ValueError as ts_error:
                                parsed_timestamp = datetime.utcnow()
                                logger.warning("‚ö†Ô∏è TIMESTAMP PARSING FAILED, USING CURRENT TIME",
                                             reading_id=reading_id,
                                             timestamp_error=str(ts_error),
                                             fallback_timestamp=str(parsed_timestamp))
                        else:
                            parsed_timestamp = datetime.utcnow()
                            logger.warning("‚ö†Ô∏è NO TIMESTAMP PROVIDED, USING CURRENT TIME",
                                         reading_id=reading_id,
                                         fallback_timestamp=str(parsed_timestamp))

                        values.append(parsed_timestamp)

                    elif col_name == 'source' and source_id:
                        available_columns.append(col_name)
                        values.append(source_id)
                        logger.debug(f"‚úÖ MAPPED source: {source_id}")

                    elif col_name in record:
                        available_columns.append(col_name)
                        values.append(record[col_name])
                        logger.debug(f"‚úÖ MAPPED {col_name}: {record[col_name]}")

                    elif col_name == 'wind_direction' and col_name not in record:
                        available_columns.append(col_name)
                        values.append(0.0)
                        logger.debug(f"‚ö†Ô∏è DEFAULT wind_direction: 0.0")

                    elif col_name == 'pressure' and col_name not in record:
                        available_columns.append(col_name)
                        values.append(1013.25)
                        logger.debug(f"‚ö†Ô∏è DEFAULT pressure: 1013.25")
                    else:
                        logger.debug(f"‚è≠Ô∏è SKIPPING UNMAPPED COLUMN: {col_name}")

                # Create placeholders and query
                placeholders = [f'${i}' for i in range(1, len(values) + 1)]

                if not available_columns:
                    logger.error("‚ùå NO VALID COLUMNS FOUND FOR WEATHER RECORD",
                               reading_id=reading_id,
                               record=record)
                    return

                query = f"""
                INSERT INTO weather_data ({', '.join(available_columns)})
                VALUES ({', '.join(placeholders[:len(values)])})
                """

                logger.info("üöÄ EXECUTING WEATHER RECORD INSERT QUERY",
                           reading_id=reading_id,
                           query=query,
                           columns=available_columns,
                           values=[str(v) for v in values],
                           value_types=[str(type(v)) for v in values])

                try:
                    result = await conn.execute(query, *values)

                    logger.info("‚úÖ WEATHER RECORD SUCCESSFULLY INSERTED TO DATABASE",
                               reading_id=reading_id,
                               source_id=source_id,
                               db_result=str(result),
                               columns_inserted=len(available_columns))

                except Exception as db_error:
                    logger.error("‚ùå DATABASE INSERT FAILED FOR WEATHER RECORD",
                                reading_id=reading_id,
                                source_id=source_id,
                                db_error=str(db_error),
                                db_error_type=type(db_error).__name__,
                                query=query,
                                values=str(values),
                                exc_info=True)
                    raise

        except Exception as e:
            logger.error("‚ùå WEATHER RECORD STORAGE FAILED",
                        reading_id=reading_id,
                        source_id=source_id,
                        record=record,
                        error=str(e),
                        error_type=type(e).__name__,
                        exc_info=True)
            raise

    async def _store_gridpoint_forecast(self, record: Dict[str, Any], source_id: str):
        """Store gridpoint forecast record to dedicated PostgreSQL table"""
        reading_id = str(uuid.uuid4())

        logger.info("üåê STORING GRIDPOINT FORECAST TO DATABASE",
                   reading_id=reading_id,
                   source_id=source_id,
                   record_keys=list(record.keys()) if record else [])

        try:
            async with self.db_manager.get_connection() as conn:
                # Query the gridpoint forecast table schema
                schema_query = """
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns
                WHERE table_name = 'noaa_gridpoint_forecast'
                AND table_schema = 'public'
                ORDER BY ordinal_position
                """

                schema_rows = await conn.fetch(schema_query)
                logger.info("‚úÖ GRIDPOINT FORECAST TABLE SCHEMA RETRIEVED",
                           reading_id=reading_id,
                           column_count=len(schema_rows))

                # Build dynamic insert based on available data and schema
                available_columns = []
                values = []

                for row in schema_rows:
                    col_name = row['column_name']

                    # Skip auto-generated columns
                    if col_name in ['id', 'created_at', 'updated_at']:
                        continue

                    # Get value from record
                    value = record.get(col_name)

                    if value is not None:
                        # Convert timestamp strings to datetime objects for TIMESTAMPTZ columns
                        if row['data_type'] in ['timestamp with time zone', 'timestamptz'] and isinstance(value, str):
                            from dateutil import parser
                            value = parser.parse(value)

                        available_columns.append(col_name)
                        values.append(value)

                if not available_columns:
                    logger.warning("‚ùå NO COLUMNS AVAILABLE FOR GRIDPOINT FORECAST INSERT",
                                 reading_id=reading_id,
                                 record=record)
                    return

                placeholders = [f'${i+1}' for i in range(len(values))]
                query = f"""
                INSERT INTO noaa_gridpoint_forecast ({', '.join(available_columns)})
                VALUES ({', '.join(placeholders)})
                """

                logger.info("üöÄ EXECUTING GRIDPOINT FORECAST INSERT",
                           reading_id=reading_id,
                           query=query,
                           columns=available_columns)

                result = await conn.execute(query, *values)

                logger.info("‚úÖ GRIDPOINT FORECAST SUCCESSFULLY INSERTED",
                           reading_id=reading_id,
                           source_id=source_id,
                           db_result=str(result),
                           columns_inserted=len(available_columns))

        except Exception as e:
            logger.error("‚ùå GRIDPOINT FORECAST STORAGE FAILED",
                        reading_id=reading_id,
                        source_id=source_id,
                        record=record,
                        error=str(e),
                        error_type=type(e).__name__,
                        exc_info=True)
            raise

    async def _handle_sensor_data(self, data: Dict[str, Any]):
        """
        Handle IoT sensor data - Updated to handle both single records and arrays

        Expected data formats:
        1. Nested format with metadata:
        {
            "metadata": {"source_id": "iot_sensor"},
            "data": {
                "sensor_id": "SENSOR_001",
                "latitude": 40.0,
                "longitude": -120.0,
                "temperature": 35.5,
                "humidity": 30.2,
                "smoke_level": 15.8,
                "co2_level": 410.5,
                "timestamp": "2025-09-22T12:00:00Z"
            }
        }

        2. Array format:
        {
            "metadata": {"source_id": "iot_sensor"},
            "data": [
                {"sensor_id": "SENSOR_001", "temperature": 35.5, ...},
                {"sensor_id": "SENSOR_002", "temperature": 32.1, ...}
            ]
        }

        3. Flat format (fallback):
        {
            "sensor_id": "SENSOR_001",
            "temperature": 35.5,
            "humidity": 30.2,
            "timestamp": "2025-09-22T12:00:00Z"
        }
        """
        try:
            logger.info("üîß PROCESSING SENSOR MESSAGE WITH ENHANCED HANDLING",
                       message_keys=list(data.keys()) if data else [],
                       message_preview=str(data)[:300])

            # Extract source_id from metadata or use source field
            metadata = data.get('metadata', {})
            source_id = metadata.get('source_id', data.get('source', 'unknown'))

            logger.info("üìä SENSOR SOURCE IDENTIFICATION",
                       source_id=source_id,
                       metadata_keys=list(metadata.keys()) if metadata else [],
                       has_metadata=bool(metadata))

            # Get sensor data from data field with enhanced format handling
            data_section = data.get('data', {})

            if not data_section:
                logger.warning("üìä DEBUG: No data section found in sensor message, checking for flat format",
                              available_keys=list(data.keys()) if data else [],
                              algorithm_impact="MEDIUM - Checking flat format fallback")

                # Check if this is flat format (sensor data at root level)
                if data.get('sensor_id') is not None:
                    logger.info("üìç SENSOR DATA FOUND AT ROOT LEVEL (FLAT FORMAT)", source_id=source_id)
                    data_section = data
                else:
                    logger.warning("‚ùå NO SENSOR DATA FOUND IN MESSAGE",
                                 source_id=source_id,
                                 has_data_field=bool(data.get('data')),
                                 has_sensor_id=bool(data.get('sensor_id')))
                    return

            # Handle both single records and arrays of records
            records_processed = 0

            if isinstance(data_section, list):
                logger.info("üìä PROCESSING SENSOR ARRAY FORMAT",
                           source_id=source_id,
                           array_length=len(data_section))

                # Array of sensor records - use TableRouter for automatic routing
                table_name = TableRouter.get_table_name(source_id)
                for i, record in enumerate(data_section):
                    if isinstance(record, dict):
                        logger.info(f"üìç PROCESSING SENSOR RECORD {i+1}/{len(data_section)}",
                                   record_keys=list(record.keys()) if record else [],
                                   sensor_id=record.get('sensor_id', 'unknown'),
                                   target_table=table_name)
                        mapped_data = TableRouter.map_fields(source_id, record)
                        await self._store_to_table(table_name, mapped_data, source_id)
                        records_processed += 1
                    else:
                        logger.warning(f"‚ö†Ô∏è SKIPPING INVALID SENSOR RECORD {i+1}",
                                     record_type=type(record).__name__,
                                     record_preview=str(record)[:100])

            elif isinstance(data_section, dict):
                logger.info("üìä PROCESSING SINGLE SENSOR RECORD FORMAT",
                           source_id=source_id,
                           record_keys=list(data_section.keys()))

                # Single sensor record - use TableRouter for automatic routing
                table_name = TableRouter.get_table_name(source_id)
                mapped_data = TableRouter.map_fields(source_id, data_section)
                await self._store_to_table(table_name, mapped_data, source_id)
                records_processed = 1
            else:
                logger.error("‚ùå UNSUPPORTED SENSOR DATA FORMAT",
                           source_id=source_id,
                           data_section_type=type(data_section).__name__,
                           data_section_preview=str(data_section)[:200])
                return

            logger.info("‚úÖ SENSOR DATA PROCESSING COMPLETED",
                       source_id=source_id,
                       records_processed=records_processed)

        except Exception as e:
            logger.error("‚ùå FAILED TO HANDLE SENSOR DATA",
                        error=str(e),
                        error_type=type(e).__name__,
                        exc_info=True)
            raise

    async def _store_sensor_record(self, record: Dict[str, Any], source_id: str):
        """Store individual sensor record to PostgreSQL with detailed logging"""
        reading_id = str(uuid.uuid4())

        logger.info("üìä STORING SENSOR RECORD TO DATABASE",
                   reading_id=reading_id,
                   source_id=source_id,
                   record_keys=list(record.keys()) if record else [],
                   record_preview=str(record)[:300])

        try:
            # Extract readings from nested structure if present
            readings = record.get('readings', {})
            logger.info("üîç EXTRACTED SENSOR READINGS",
                       reading_id=reading_id,
                       readings_keys=list(readings.keys()) if readings else [],
                       has_nested_readings=bool(readings))

            logger.info("üîó OBTAINING DATABASE CONNECTION FOR SENSOR RECORD",
                       reading_id=reading_id)

            async with self.db_manager.get_connection() as conn:
                logger.info("‚úÖ DATABASE CONNECTION OBTAINED FOR SENSOR RECORD",
                           reading_id=reading_id)

                # Query the table schema to get column names and types
                logger.info("üîç QUERYING SENSOR_READINGS TABLE SCHEMA",
                           reading_id=reading_id)

                schema_query = """
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns
                WHERE table_name = 'sensor_readings'
                AND table_schema = 'public'
                ORDER BY ordinal_position
                """

                try:
                    schema_rows = await conn.fetch(schema_query)
                    logger.info("‚úÖ SENSOR TABLE SCHEMA RETRIEVED",
                               reading_id=reading_id,
                               column_count=len(schema_rows),
                               columns=[row['column_name'] for row in schema_rows])
                except Exception as schema_error:
                    logger.error("‚ùå FAILED TO RETRIEVE SENSOR TABLE SCHEMA",
                                reading_id=reading_id,
                                schema_error=str(schema_error),
                                schema_error_type=type(schema_error).__name__)
                    raise

                # Build dynamic insert based on available data and schema
                available_columns = []
                values = []

                logger.info("‚öôÔ∏è BUILDING DYNAMIC INSERT FOR SENSOR RECORD",
                           reading_id=reading_id)

                for row in schema_rows:
                    col_name = row['column_name']
                    is_nullable = row['is_nullable'] == 'YES'

                    logger.debug(f"üìù PROCESSING SENSOR COLUMN: {col_name}",
                               reading_id=reading_id,
                               data_type=row['data_type'],
                               is_nullable=is_nullable)

                    # Skip auto-generated columns
                    if col_name in ['created_at']:
                        logger.debug(f"‚è≠Ô∏è SKIPPING AUTO-GENERATED COLUMN: {col_name}")
                        continue

                    # Handle required ID column
                    if col_name == 'id':
                        available_columns.append(col_name)
                        values.append(reading_id)
                        logger.debug(f"‚úÖ MAPPED id: {reading_id}")
                        continue

                    # Map data fields to database columns
                    value = None
                    if col_name == 'reading_id':
                        value = reading_id
                        logger.debug(f"‚úÖ MAPPED reading_id: {reading_id}")

                    elif col_name == 'timestamp':
                        value = self._parse_timestamp(record)
                        logger.info("‚è∞ PROCESSED SENSOR TIMESTAMP",
                                   reading_id=reading_id,
                                   original=record.get('timestamp'),
                                   parsed=str(value),
                                   type=str(type(value)))

                    elif col_name == 'source' and source_id:
                        value = source_id
                        logger.debug(f"‚úÖ MAPPED source: {source_id}")

                    elif col_name in record:
                        value = record[col_name]
                        logger.debug(f"‚úÖ MAPPED {col_name} from record: {value}")

                    elif col_name in readings:
                        value = readings[col_name]
                        logger.debug(f"‚úÖ MAPPED {col_name} from readings: {value}")

                    # Handle special database column mappings that don't match sensor data fields
                    elif col_name == 'co2_level' and 'co2' in record:
                        value = record['co2']
                        logger.debug(f"‚úÖ MAPPED {col_name} from co2: {value}")
                    elif col_name == 'co2_level' and 'co2_level' in record:
                        value = record['co2_level']
                        logger.debug(f"‚úÖ MAPPED {col_name} from co2_level: {value}")

                    # Only include columns that have values or are nullable
                    if value is not None or is_nullable:
                        available_columns.append(col_name)
                        values.append(value)
                        logger.debug(f"‚úÖ INCLUDED {col_name}: {value} (nullable={is_nullable})")
                    else:
                        logger.debug(f"‚è≠Ô∏è SKIPPED {col_name}: no value and not nullable")

                        # Enhanced logging for debugging column mismatches
                        if col_name not in ['id', 'created_at', 'reading_id', 'timestamp', 'source']:
                            logger.info(f"üîç SENSOR COLUMN ANALYSIS: {col_name} not found in data",
                                       available_in_record=list(record.keys()) if record else [],
                                       available_in_readings=list(readings.keys()) if readings else [],
                                       column_type=row['data_type'],
                                       is_nullable=is_nullable)

                # Create placeholders and query
                placeholders = [f'${i}' for i in range(1, len(values) + 1)]

                if not available_columns:
                    logger.error("‚ùå NO VALID COLUMNS FOUND FOR SENSOR RECORD",
                               reading_id=reading_id,
                               record_keys=list(record.keys()) if record else [],
                               readings_keys=list(readings.keys()) if readings else [],
                               record_preview=str(record)[:300],
                               algorithm_impact="HIGH - Sensor record completely rejected")

                    # Additional debugging for column mapping failures
                    logger.error("üîç SENSOR COLUMN MAPPING FAILURE ANALYSIS",
                               record_fields=list(record.keys()) if record else [],
                               expected_database_columns=[row['column_name'] for row in schema_rows],
                               missing_required_fields="Check if sensor_id, latitude, longitude, temperature exist in data")
                    return

                query = f"""
                INSERT INTO sensor_readings ({', '.join(available_columns)})
                VALUES ({', '.join(placeholders[:len(values)])})
                """

                logger.info("üöÄ EXECUTING SENSOR RECORD INSERT QUERY",
                           reading_id=reading_id,
                           query=query,
                           columns=available_columns,
                           values=[str(v) for v in values],
                           value_types=[str(type(v)) for v in values])

                try:
                    result = await conn.execute(query, *values)

                    logger.info("‚úÖ SENSOR RECORD SUCCESSFULLY INSERTED TO DATABASE",
                               reading_id=reading_id,
                               source_id=source_id,
                               db_result=str(result),
                               columns_inserted=len(available_columns))

                except Exception as db_error:
                    logger.error("‚ùå DATABASE INSERT FAILED FOR SENSOR RECORD",
                                reading_id=reading_id,
                                source_id=source_id,
                                db_error=str(db_error),
                                db_error_type=type(db_error).__name__,
                                query=query,
                                values=str(values),
                                exc_info=True)
                    raise

        except Exception as e:
            logger.error("‚ùå SENSOR RECORD STORAGE FAILED",
                        reading_id=reading_id,
                        source_id=source_id,
                        record=record,
                        error=str(e),
                        error_type=type(e).__name__,
                        exc_info=True)
            raise

    def _parse_timestamp(self, data: Dict[str, Any]) -> datetime:
        """Parse timestamp from various formats"""
        try:
            # Try different timestamp formats
            if 'timestamp' in data:
                timestamp_str = data['timestamp']
                if isinstance(timestamp_str, str):
                    # Try ISO format first (handles both 'Z' and timezone offsets)
                    try:
                        if timestamp_str.endswith('Z'):
                            # Replace 'Z' with explicit UTC offset
                            timestamp_str = timestamp_str[:-1] + '+00:00'
                        # Parse ISO format timestamp
                        dt = datetime.fromisoformat(timestamp_str)
                        # Ensure we return a datetime object (not string)
                        return dt.replace(tzinfo=None) if dt.tzinfo else dt
                    except ValueError:
                        # Try parsing without timezone info
                        try:
                            clean_str = timestamp_str.replace('Z', '').replace('T', ' ')
                            return datetime.strptime(clean_str, '%Y-%m-%d %H:%M:%S.%f')
                        except ValueError:
                            try:
                                return datetime.strptime(clean_str, '%Y-%m-%d %H:%M:%S')
                            except ValueError:
                                pass

            # Try NASA FIRMS format (acq_date + acq_time)
            if 'acq_date' in data and 'acq_time' in data:
                date_str = data['acq_date']  # YYYY-MM-DD
                time_str = str(data['acq_time'])  # HHMM (ensure it's a string)

                if len(time_str) == 4 and time_str.isdigit():
                    hour = int(time_str[:2])
                    minute = int(time_str[2:])

                    date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    return date_obj.replace(hour=hour, minute=minute)

            # Default to current time if parsing fails
            logger.warning("Could not parse timestamp, using current time", data=data)
            return datetime.utcnow()

        except Exception as e:
            logger.warning("Timestamp parsing failed, using current time", error=str(e), data=data)
            return datetime.utcnow()

    def _parse_sophisticated_timestamp(self, detection: Dict[str, Any]) -> datetime:
        """Parse timestamp from sophisticated main system format"""
        try:
            # Main system uses acq_date (YYYY-MM-DD) and acq_time (HHMM)
            if 'acq_date' in detection and 'acq_time' in detection:
                date_str = detection['acq_date']  # "2025-09-22"
                time_str = str(detection['acq_time'])  # "1643"

                if len(time_str) == 4 and time_str.isdigit():
                    hour = int(time_str[:2])
                    minute = int(time_str[2:])

                    # Parse date and add time
                    date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                    return date_obj.replace(hour=hour, minute=minute)

            # Fallback to original timestamp parsing
            return self._parse_timestamp(detection)

        except Exception as e:
            logger.warning("Sophisticated timestamp parsing failed, using current time", error=str(e))
            return datetime.utcnow()

    async def _handle_satellite_data(self, data: Dict[str, Any]):
        """Handle general satellite data including Sentinel-2 and Sentinel-3"""
        try:
            metadata = data.get('metadata', {})
            # Check source_id at top level first, then in metadata, then fall back to source
            source_id = data.get('source_id') or metadata.get('source_id') or data.get('source', 'unknown')

            logger.info("Received satellite data",
                       source_id=source_id,
                       data_keys=list(data.keys()) if data else [])

            # Route to appropriate handler based on source
            if source_id == 'sat_sentinel2_msi':
                await self._store_sentinel2_data(data)
            elif source_id == 'sat_sentinel3_slstr':
                await self._store_sentinel3_data(data)
            else:
                logger.info("Satellite data logged (no handler for source)", source_id=source_id)

        except Exception as e:
            logger.error("Failed to handle satellite data", error=str(e))
            raise

    async def _store_sentinel2_data(self, data: Dict[str, Any]):
        """Store Sentinel-2 MSI data in sentinel2_msi_imagery table"""
        try:
            # Extract fields from data
            product_id = data.get('product_id')
            timestamp_str = data.get('timestamp')

            if not product_id or not timestamp_str:
                logger.warning("Missing required Sentinel-2 fields", data=data)
                return

            # Parse timestamp string to datetime
            from dateutil import parser
            timestamp = parser.isoparse(timestamp_str)

            # Check if s3_path is available (downloaded imagery)
            s3_path = data.get('s3_path')

            # Insert query with ON CONFLICT to handle duplicates
            if s3_path:
                query = """
                    INSERT INTO sentinel2_msi_imagery (
                        timestamp, product_id, cloud_cover_percent,
                        satellite, sensor, download_url, s3_path
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6, $7
                    )
                    ON CONFLICT (product_id) DO UPDATE SET s3_path = EXCLUDED.s3_path
                """
                params = (
                    timestamp,
                    product_id,
                    data.get('cloud_cover', 0.0),
                    data.get('satellite', 'Sentinel-2'),
                    data.get('instrument', 'MSI'),
                    data.get('download_url', ''),
                    s3_path
                )
            else:
                query = """
                    INSERT INTO sentinel2_msi_imagery (
                        timestamp, product_id, cloud_cover_percent,
                        satellite, sensor, download_url
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6
                    )
                    ON CONFLICT (product_id) DO NOTHING
                """
                params = (
                    timestamp,
                    product_id,
                    data.get('cloud_cover', 0.0),
                    data.get('satellite', 'Sentinel-2'),
                    data.get('instrument', 'MSI'),
                    data.get('download_url', '')
                )

            async with self.db_manager.get_connection() as conn:
                await conn.execute(query, *params)

            logger.info("Sentinel-2 data stored successfully",
                       product_id=product_id,
                       has_s3_path=bool(s3_path))

        except Exception as e:
            logger.error("Failed to store Sentinel-2 data", error=str(e), data=data)
            raise

    async def _store_sentinel3_data(self, data: Dict[str, Any]):
        """Store Sentinel-3 SLSTR data in sentinel3_slstr_imagery table"""
        try:
            # Extract fields from data
            product_id = data.get('product_id')
            timestamp_str = data.get('timestamp')

            if not product_id or not timestamp_str:
                logger.warning("Missing required Sentinel-3 fields", data=data)
                return

            # Parse timestamp string to datetime
            from dateutil import parser
            timestamp = parser.isoparse(timestamp_str)

            # Check if s3_path is available (downloaded imagery)
            s3_path = data.get('s3_path')

            # Insert query with ON CONFLICT to handle duplicates
            if s3_path:
                query = """
                    INSERT INTO sentinel3_slstr_imagery (
                        timestamp, product_id, satellite, sensor,
                        download_url, file_size_mb, s3_path
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6, $7
                    )
                    ON CONFLICT (product_id) DO UPDATE SET s3_path = EXCLUDED.s3_path
                """
                params = (
                    timestamp,
                    product_id,
                    data.get('satellite', 'Sentinel-3'),
                    data.get('instrument', 'SLSTR'),
                    data.get('download_url', ''),
                    data.get('size_mb', 0.0),
                    s3_path
                )
            else:
                query = """
                    INSERT INTO sentinel3_slstr_imagery (
                        timestamp, product_id, satellite, sensor,
                        download_url, file_size_mb
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6
                    )
                    ON CONFLICT (product_id) DO NOTHING
                """
                params = (
                    timestamp,
                    product_id,
                    data.get('satellite', 'Sentinel-3'),
                    data.get('instrument', 'SLSTR'),
                    data.get('download_url', ''),
                    data.get('size_mb', 0.0)
                )

            async with self.db_manager.get_connection() as conn:
                await conn.execute(query, *params)

            logger.info("Sentinel-3 data stored successfully",
                       product_id=product_id,
                       has_s3_path=bool(s3_path))

        except Exception as e:
            logger.error("Failed to store Sentinel-3 data", error=str(e), data=data)
            raise

    async def _handle_satellite_imagery(self, data: Dict[str, Any]):
        """Handle satellite imagery data - for now, log and skip as we don't have a specific table"""
        try:
            metadata = data.get('metadata', {})
            source_id = metadata.get('source_id', data.get('source', 'unknown'))

            logger.info("Received satellite imagery data - logging only",
                       source_id=source_id,
                       data_keys=list(data.keys()) if data else [])

            # TODO: Create satellite_imagery table if needed for binary/image data
            logger.info("Satellite imagery data logged (no database table configured)", source_id=source_id)

        except Exception as e:
            logger.error("Failed to handle satellite imagery data", error=str(e))
            raise


class KafkaConsumerManager:
    """
    Manager for Kafka consumer lifecycle

    Handles starting/stopping the consumer and managing background tasks.
    """

    def __init__(self, db_manager: DatabaseManager, auto_offset_reset: str = None):
        self.db_manager = db_manager
        self.consumer = None
        self.consumer_task = None
        self.auto_offset_reset = auto_offset_reset

    async def start(self):
        """Start Kafka consumer in background"""
        if self.consumer_task is not None:
            logger.warning("Kafka consumer already running")
            return

        try:
            self.consumer = KafkaDataConsumer(self.db_manager, self.auto_offset_reset)

            # Start consumer in background task
            self.consumer_task = asyncio.create_task(self.consumer.start())

            logger.info("Kafka consumer manager started")

        except Exception as e:
            logger.error("Failed to start Kafka consumer manager", error=str(e))
            raise

    async def stop(self):
        """Stop Kafka consumer"""
        if self.consumer:
            await self.consumer.stop()

        if self.consumer_task:
            self.consumer_task.cancel()
            try:
                await self.consumer_task
            except asyncio.CancelledError:
                pass

        logger.info("Kafka consumer manager stopped")

    def is_running(self) -> bool:
        """Check if consumer is running"""
        return (self.consumer_task is not None and
                not self.consumer_task.done() and
                self.consumer and
                self.consumer.running)