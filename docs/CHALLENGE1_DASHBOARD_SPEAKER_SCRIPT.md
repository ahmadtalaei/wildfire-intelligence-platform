# Challenge 1: Data Sources and Ingestion Latency & Fidelity Monitoring

## Setup Instructions - Before We Begin

Welcome to the Challenge One monitoring dashboard for the Wildfire Intelligence Platform…


Before we dive into the dashboard… let me walk you through how to access this monitoring interface…

First… open your web browser and navigate to localhost port three thousand ten…

You'll see the Grafana login page…

Enter the username… admin… all lowercase…

Then the password… also admin… all lowercase…

Click Sign In to access the Grafana dashboard interface.


Once logged in… you have two options to view the Challenge One dashboard…

Option one… if the dashboard is already imported… simply click on Dashboards in the left sidebar…

Then search for Challenge One Ingestion… or scroll through the list to find it…

Click on the dashboard name to open it.


Option two… if you need to import the dashboard from the JSON file…

Click the plus icon in the left sidebar… then select Import dashboard…

Click Upload JSON file… and browse to the file location…

Navigate to monitoring slash grafana slash dashboards slash challenge one latency dashboard json file…

Select the file and click Import… If the dashboard is already imported, Grafana may display an error stating... A dashboard or folder with the same name already exists...

Once imported successfully, Grafana will load the dashboard configuration… and you'll see the Challenge One monitoring interface appear.

This dashboard provides real-time visibility into our data ingestion pipeline… showing how quickly we capture data from satellites, weather stations, and sensors… and how accurately we validate each record before it enters our system.


First, set the time range to Last one hour using the time picker in the upper right corner…

This shows data from the most recent hour of operation…

Please note that the metrics you're seeing represent recent running ingestion and consumption processes…

This dashboard provides real-time visibility into our data ingestion pipeline… showing how quickly we capture data from satellites, weather stations, and sensors… and how accurately we validate each record before it enters our system.

The dashboard updates every ten seconds… so you're seeing near real-time performance metrics as data continues to stream through the platform.


Now… let's explore what this dashboard shows us…


## Top Row: Real-Time Ingestion Latency

At the top of the dashboard, we see ingestion latency metrics broken down by data source…

The first panel shows three key percentiles… p fifty, p ninety-five, and p ninety-nine…

For example, it's currently showing latency around one to two point five seconds… meaning ninety-five percent of data reach our Kafka streaming platform in under two and a half seconds… Overall, p ninety-five latency staying well under five seconds…

These metrics update every ten seconds… giving us instant feedback if any data source starts experiencing delays.


## Validation Pass Rate by Source

Moving to the next panel… we monitor validation pass rate for each data source…

This shows what percentage of incoming records successfully pass our schema validation and quality checks…

Right now, all sources are showing one hundred percent validation success…

This tells us our data connectors are properly formatted and our validation rules are working correctly… 

## Second Row: Throughput and Consumer Lag

Moving down to the second row… we see the Ingestion Throughput panel on the left…

This time-series chart displays the rate of records ingested per second… broken down by data source…

You can see different colored lines representing each source… with NOAA weather stations typically showing higher throughput due to frequent updates…

NASA FIRMS satellite sources show variable throughput… spiking when new satellite passes detect active fires…

The legend shows both individual source throughput and a TOTAL line combining all sources… giving us an overall view of system load.


On the right side… the Kafka Consumer Lag gauge shows how many messages are waiting to be processed…

We're tracking four main topics here… nasa-firms, weather-data, weather-alerts, and weather-stations…

Currently, weather-data shows some messages in the queue… while the other topics show minimal or zero lag…

This is healthy… it means our consumers are keeping up with the incoming data stream… processing records almost as fast as they arrive.


## Third Row: SLA Compliance Indicators

In the third row… we have four Service Level Agreement indicators…

The first one… SLA Validation Success Rate… shows one hundred percent with a green background…

This means every record we attempted to validate has passed successfully… meeting our SLA target of ninety-five percent or higher…

Next to it… SLA p ninety-five Latency less than five second… also shows green…

Our current p ninety-five latency is around three seconds… giving us plenty of buffer for processing time…

The third panel tracks duplicate rate… showing zero percent…

This confirms our S H A two fifty-six deduplication system is working… preventing the same fire detection or weather reading from being ingested twice…

Finally… the overall Success Rate panel shows near one hundred percent… calculated by dividing validation passed total by validation total… indicating our end-to-end ingestion pipeline is performing reliably.


## Recent Failed Messages

Below the SLA indicators… we see a table titled Recent Failed Messages…

This panel is configured to show the top twenty error types from the last five minutes…

If errors do occur, they'll appear here organized by error type and source… making it easy to spot patterns and troubleshoot issues quickly.

Right now, it's showing "noaa_stations_current" source…  with validation failures of Null Data type. These null data do not ingest to our data bases.



## Bottom Section: Data Quality and Anomaly Detection

At the bottom of the dashboard… we have two additional monitoring panels…

The first panel shows Data Quality Score by Source… displayed as a time-series chart…

This tracks the quality score for each data source over time… with scores ranging from zero to one…

A score of one represents perfect data quality… with all fields present, valid formats, and no anomalies…

You can see most sources maintaining scores above zero point nine… indicating high quality data ingestion…

Any dips in the quality score would alert us to potential issues with a data source… such as missing fields or corrupted records.


Next to it… the Anomalies Detected by Source and Type panel shows another time-series view…

This displays the count of anomalies detected over time… broken down by source and anomaly type…

Anomalies might include unusual values… unexpected data patterns… or records that fail business logic validation…

Currently… we're seeing very low or zero anomaly counts… which confirms our validation and quality control systems are working effectively…

If anomalies spike… operators can investigate the specific source and time range to identify the root cause.


## Real-Time Updates

This entire dashboard refreshes every ten seconds…

So as new satellite images come in… or weather stations report updated conditions… you'll see the metrics update in near real-time…

The latency percentiles recalculate continuously… the validation pass rate gauges adjust as new records are processed… the SLA indicators re-evaluate thresholds… and the Kafka consumer lag counters update based on current queue depth…

The quality scores and anomaly detection panels show trending data… allowing you to spot patterns over the last hour…

This gives us continuous visibility into the health of our ingestion pipeline… allowing us to detect and respond to issues within seconds… not minutes or hours.


## Why This Matters for Challenge One

Challenge One assesses our ingestion architecture based on three critical criteria: latency, fidelity, and reliability... This dashboard provides a clear view of all three...

First… latency is measured end-to-end from data source to Kafka… with p ninety-five consistently under three seconds…

Second… fidelity is proven through our one hundred percent validation pass rate… showing we're capturing data accurately without corruption…

And Reliability... minimal consumer lag indicates the system handles thousands of records per hour efficiently. Occasional validation failures, such as null data from noaa_stations_current, are identified in the Failed Messages table but are not ingested into the database.

The fact that all SLA indicators are green… shows we've built a production-ready ingestion system that meets enterprise standards for wildfire emergency response.
