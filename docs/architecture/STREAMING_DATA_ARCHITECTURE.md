# ğŸŒŠ Robust Streaming Data Architecture for Wildfire Intelligence Platform

## Executive Summary

This document proposes a **production-grade, scalable streaming data architecture** for the CAL FIRE Wildfire Intelligence Platform. This architecture addresses:

âœ… High-frequency streaming of satellite/fire/weather data
âœ… Large-scale datasets (images, rasters, time series)
âœ… Historical storage without excessive cleanup
âœ… Dynamic scalability for fire season peaks
âœ… Efficient query performance
âœ… Low operational maintenance

---

## 1. Current Architecture Analysis

### âœ… Strengths
- **Kafka-based event streaming** already implemented
- **Multi-tier storage** (PostgreSQL, Redis, MinIO, Elasticsearch)
- **Microservices architecture** with clear separation of concerns
- **Docker-based deployment** for portability

### âš ï¸ Gaps Identified
1. **No automated data retention policies** â†’ Manual cleanup required (as we just experienced!)
2. **No tiered storage lifecycle** â†’ All data stays in hot storage
3. **Limited batch processing** â†’ No historical data archival automation
4. **No object storage partitioning** â†’ 205GB of Sentinel data with no organization
5. **No streaming data compaction** â†’ Kafka retention set to 7 days only
6. **Limited metadata catalog** â†’ Hard to query historical data efficiently

---

## 2. Recommended Production Architecture

### 2.A. **Streaming Ingestion Layer** (Already Partially Implemented)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA SOURCES (Producers)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MODIS/VIIRS  â”‚  ERA5/NAM  â”‚  RAWS  â”‚  Sentinel  â”‚  IoT Sensors  â”‚
â”‚  (Kafka)     â”‚  (Kafka)   â”‚ (Kafka)â”‚   (MinIO)  â”‚    (MQTT)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KAFKA CLUSTER (Event Streaming)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Topics:                                                          â”‚
â”‚  â€¢ firms.viirs.detections    (Retention: 30 days)                â”‚
â”‚  â€¢ weather.era5.hourly       (Retention: 90 days)                â”‚
â”‚  â€¢ weather.nam.forecasts     (Retention: 7 days)                 â”‚
â”‚  â€¢ sensors.iot.readings      (Retention: 30 days)                â”‚
â”‚  â€¢ satellite.sentinel.raw    (Retention: 1 day â†’ Move to MinIO)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Configuration:                                                   â”‚
â”‚  â€¢ Compression: gzip/snappy/lz4                                  â”‚
â”‚  â€¢ Partitioning: By region (CA-North, CA-South, CA-Central)      â”‚
â”‚  â€¢ Replication Factor: 3 (for production)                        â”‚
â”‚  â€¢ Min In-Sync Replicas: 2                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Improvements:**
- **Topic-specific retention**: Don't keep forecasts as long as observations
- **Compression enabled**: Reduce storage by 70-80%
- **Partitioning by region**: Parallel processing, easier queries
- **Large files (Sentinel) bypass Kafka**: Go directly to MinIO with metadata in Kafka

---

### 2.B. **Storage Layer - Tiered Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIERED STORAGE STRATEGY                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HOT TIER (0-7 days) - Real-Time Access                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Redis (In-Memory Cache) - 10GB                                 â”‚
â”‚   - Active fire detections                                       â”‚
â”‚   - Current weather readings                                     â”‚
â”‚   - Risk scores (last 24 hours)                                  â”‚
â”‚   - User sessions                                                â”‚
â”‚   TTL: 1 hour (auto-expire)                                      â”‚
â”‚                                                                  â”‚
â”‚ â€¢ PostgreSQL (Relational + TimescaleDB)                          â”‚
â”‚   - Fire detections (indexed by time + location)                 â”‚
â”‚   - Weather observations (hypertable, 1-day chunks)              â”‚
â”‚   - Sensor readings (compressed, 1-hour chunks)                  â”‚
â”‚   - Forecast data (compressed)                                   â”‚
â”‚   Retention: 7 days â†’ Auto-move to WARM                          â”‚
â”‚   Compression: Enabled on TimescaleDB hypertables                â”‚
â”‚   Size Estimate: 15-20GB                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WARM TIER (7-90 days) - Analytical Queries                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ MinIO (Object Storage) - Partitioned by Date                   â”‚
â”‚   s3://wildfire-data/                                            â”‚
â”‚   â”œâ”€â”€ weather/                                                   â”‚
â”‚   â”‚   â”œâ”€â”€ era5/YYYY/MM/DD/era5_YYYYMMDD.parquet                  â”‚
â”‚   â”‚   â”œâ”€â”€ nam/YYYY/MM/DD/nam_YYYYMMDD.parquet                    â”‚
â”‚   â”œâ”€â”€ fires/                                                     â”‚
â”‚   â”‚   â”œâ”€â”€ viirs/YYYY/MM/DD/viirs_YYYYMMDD.parquet                â”‚
â”‚   â”‚   â”œâ”€â”€ modis/YYYY/MM/DD/modis_YYYYMMDD.parquet                â”‚
â”‚   â”œâ”€â”€ satellite/                                                 â”‚
â”‚   â”‚   â”œâ”€â”€ sentinel2/YYYY/MM/DD/tile_name.tif (compressed)        â”‚
â”‚   â”‚   â”œâ”€â”€ landsat8/YYYY/MM/DD/scene_id.tif (COG format)          â”‚
â”‚   Compression: Parquet (70% reduction), GeoTIFF with LZW          â”‚
â”‚   Size Estimate: 50-100GB                                        â”‚
â”‚                                                                  â”‚
â”‚ â€¢ PostgreSQL (Aggregated Data)                                   â”‚
â”‚   - Daily fire summaries                                         â”‚
â”‚   - Hourly weather aggregates                                    â”‚
â”‚   Retention: 90 days â†’ Auto-move to COLD                         â”‚
â”‚   Size Estimate: 5-10GB                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COLD TIER (90-365 days) - Historical Analysis                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ MinIO with Intelligent Tiering (or Cloud S3 Standard-IA)       â”‚
â”‚   s3://wildfire-archive/                                         â”‚
â”‚   â”œâ”€â”€ weather/YYYY/quarter/Q1_weather.parquet                    â”‚
â”‚   â”œâ”€â”€ fires/YYYY/month/MM_fires.parquet                          â”‚
â”‚   â”œâ”€â”€ satellite/YYYY/month/tiles/ (sparse, key scenes only)      â”‚
â”‚   Compression: Delta Lake format with Z-ordering                 â”‚
â”‚   Access: 5-10 second query latency                              â”‚
â”‚   Size Estimate: 200-500GB                                       â”‚
â”‚                                                                  â”‚
â”‚ â€¢ Delta Lake / Iceberg (Optional for Advanced Analytics)         â”‚
â”‚   - ACID transactions on object storage                          â”‚
â”‚   - Time travel capabilities                                     â”‚
â”‚   - Incremental updates                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARCHIVE TIER (>365 days) - Compliance & Research                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Cloud Storage (AWS Glacier / Azure Archive)                    â”‚
â”‚   - Multi-year fire history                                      â”‚
â”‚   - Satellite imagery mosaics (annual composites)                â”‚
â”‚   - Model training datasets                                      â”‚
â”‚   Retrieval: 1-12 hours                                          â”‚
â”‚   Cost: $0.001/GB/month                                          â”‚
â”‚   Size Estimate: Unlimited (grows 100GB/year)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.C. **Processing Layer - Stream + Batch**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STREAM PROCESSING (Real-Time)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Kafka Streams / Apache Flink                                  â”‚
â”‚   â”œâ”€â”€ Fire Detection Enrichment                                 â”‚
â”‚   â”‚   - Join weather data with fire detections                  â”‚
â”‚   â”‚   - Calculate Fire Weather Index (FWI)                      â”‚
â”‚   â”‚   - Geospatial clustering (nearby fires)                    â”‚
â”‚   â”œâ”€â”€ Anomaly Detection                                         â”‚
â”‚   â”‚   - Sudden temperature spikes                               â”‚
â”‚   â”‚   - Humidity drops                                          â”‚
â”‚   â”‚   - Wind speed increases                                    â”‚
â”‚   â”œâ”€â”€ Real-Time Risk Scoring                                    â”‚
â”‚   â”‚   - ML model inference (ensemble)                           â”‚
â”‚   â”‚   - Alert generation (high-risk areas)                      â”‚
â”‚   Output: Processed events â†’ PostgreSQL + Redis + UI WebSocket  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BATCH PROCESSING (Historical ETL)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Apache Airflow (Orchestration) + Spark/Dask                   â”‚
â”‚   â”œâ”€â”€ Daily Jobs (12:00 AM PST)                                 â”‚
â”‚   â”‚   - Move HOT â†’ WARM tier (PostgreSQL â†’ MinIO Parquet)       â”‚
â”‚   â”‚   - Compress and partition data                             â”‚
â”‚   â”‚   - Generate daily fire summary reports                     â”‚
â”‚   â”œâ”€â”€ Weekly Jobs (Sundays)                                     â”‚
â”‚   â”‚   - Move WARM â†’ COLD tier (MinIO â†’ Cloud S3-IA)             â”‚
â”‚   â”‚   - Delete old Sentinel tiles (keep only key scenes)        â”‚
â”‚   â”‚   - Retrain ML models on updated data                       â”‚
â”‚   â”œâ”€â”€ Monthly Jobs                                              â”‚
â”‚   â”‚   - Move COLD â†’ ARCHIVE tier (S3-IA â†’ Glacier)              â”‚
â”‚   â”‚   - Create monthly data snapshots                           â”‚
â”‚   â”‚   - Audit data quality metrics                              â”‚
â”‚   Output: Processed data in MinIO/Cloud, metadata in catalog    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Airflow DAG Example:**
```python
# airflow/dags/daily_data_lifecycle.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def move_hot_to_warm():
    """Move 7-day-old data from PostgreSQL to MinIO Parquet"""
    # Query PostgreSQL for data older than 7 days
    # Export to Parquet with date partitioning
    # Upload to MinIO: s3://wildfire-data/weather/YYYY/MM/DD/
    # Delete from PostgreSQL after verification
    pass

dag = DAG(
    'daily_data_lifecycle',
    schedule_interval='0 0 * * *',  # Daily at midnight
    start_date=datetime(2025, 1, 1),
    catchup=False
)

hot_to_warm = PythonOperator(
    task_id='move_hot_to_warm',
    python_callable=move_hot_to_warm,
    dag=dag
)
```

---

### 2.D. **Metadata & Indexing Layer**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    METADATA CATALOG                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ PostgreSQL Metadata Tables                                    â”‚
â”‚   â”œâ”€â”€ data_catalog                                              â”‚
â”‚   â”‚   - file_path, timestamp, data_source, size_bytes           â”‚
â”‚   â”‚   - min_lat, max_lat, min_lon, max_lon (spatial bounds)     â”‚
â”‚   â”‚   - record_count, quality_score                             â”‚
â”‚   â”œâ”€â”€ satellite_scenes                                          â”‚
â”‚   â”‚   - scene_id, satellite, cloud_cover_percent                â”‚
â”‚   â”‚   - ingestion_date, storage_tier (HOT/WARM/COLD/ARCHIVE)    â”‚
â”‚   â”œâ”€â”€ data_lineage                                              â”‚
â”‚   â”‚   - source_id, transformation_job, output_id, timestamp     â”‚
â”‚   Output: Fast metadata queries without scanning object storage â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY OPTIMIZATION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Spatial Indexing (PostGIS)                                    â”‚
â”‚   - Bounding box queries for fire/weather by region             â”‚
â”‚   - KNN searches (nearest weather station to fire)              â”‚
â”‚ â€¢ Time Indexing (TimescaleDB)                                   â”‚
â”‚   - Optimized for time-range queries                            â”‚
â”‚   - Continuous aggregates (pre-computed hourly/daily stats)     â”‚
â”‚ â€¢ Elasticsearch (Future)                                        â”‚
â”‚   - Full-text search on fire incident reports                   â”‚
â”‚   - Aggregations for analytics dashboards                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.E. **Automated Retention & Lifecycle Policies**

```yaml
# config/data_retention_policies.yaml
policies:
  fire_detections:
    hot_tier: 7 days
    warm_tier: 90 days
    cold_tier: 365 days
    archive_tier: 7 years  # CAL FIRE compliance
    deletion: never  # Keep forever for research

  weather_forecasts:
    hot_tier: 3 days
    warm_tier: 30 days
    cold_tier: 90 days
    archive_tier: 1 year
    deletion: after_archive  # Can delete old forecasts

  weather_observations:
    hot_tier: 7 days
    warm_tier: 90 days
    cold_tier: 365 days
    archive_tier: 10 years
    deletion: never

  satellite_imagery:
    sentinel2:
      hot_tier: 1 day  # Move immediately after processing
      warm_tier: 30 days  # Keep recent clear-sky scenes
      cold_tier: 90 days  # Keep only <10% cloud cover scenes
      archive_tier: key_scenes_only  # Fire events + clear composites
      deletion_criteria:
        cloud_cover: ">30%"
        age: ">90 days"
        not_tagged_as: "fire_event"

    landsat8:
      # Similar policy but 16-day revisit, so more selective
      warm_tier: 60 days
      cold_tier: 180 days
      archive_tier: annual_composites

  sensor_readings:
    hot_tier: 1 day
    warm_tier: 30 days
    cold_tier: 365 days
    archive_tier: aggregated_only  # Keep hourly averages, not raw
    deletion: after_aggregation

automation:
  airflow_dags:
    - daily_hot_to_warm_migration
    - weekly_warm_to_cold_migration
    - monthly_cold_to_archive_migration
    - quarterly_data_quality_audit

  alerts:
    - storage_usage_80_percent
    - retention_policy_failures
    - data_quality_degradation
```

---

## 3. Data Formats & Compression Strategy

### Recommended Formats by Data Type

| Data Type | Raw Format | Storage Format | Compression | Size Reduction |
|-----------|------------|----------------|-------------|----------------|
| Fire detections (points) | CSV/JSON | Parquet | Snappy | 80% |
| Weather grids (NetCDF) | NetCDF4 | Zarr/Parquet | Blosc/Snappy | 70% |
| Satellite imagery | GeoTIFF | Cloud-Optimized GeoTIFF (COG) | LZW/Deflate | 50-70% |
| Time series (sensors) | JSON | Parquet (columnar) | Snappy | 85% |
| Metadata | JSON | PostgreSQL JSONB | Built-in | 40% |

**Why Parquet?**
- Columnar storage â†’ Only read columns you need
- Built-in compression â†’ 70-90% size reduction
- Schema evolution â†’ Add fields without breaking existing data
- Native support in Spark, Pandas, DuckDB

**Why Cloud-Optimized GeoTIFF (COG)?**
- HTTP range requests â†’ Fetch only needed image tiles
- Internal tiling â†’ Fast partial reads
- Overviews â†’ Multi-resolution pyramids for fast rendering

---

## 4. Scalability & Cost Optimization

### 4.A. Storage Cost Projection (Per Month)

```
CURRENT (No Lifecycle Policies):
â”œâ”€â”€ PostgreSQL (5GB)           $50  (SSD-backed RDS)
â”œâ”€â”€ MinIO HOT (205GB)          $100 (Local NVMe)
â”œâ”€â”€ Elasticsearch (5GB)        $25  (Managed service)
â”œâ”€â”€ Redis (10GB)               $80  (In-memory)
â””â”€â”€ TOTAL:                     $255/month

AFTER TIERED ARCHITECTURE:
â”œâ”€â”€ PostgreSQL HOT (15GB)      $75  (7 days only)
â”œâ”€â”€ Redis Cache (10GB)         $80  (1 hour TTL)
â”œâ”€â”€ MinIO WARM (80GB Parquet)  $40  (7-90 days, compressed)
â”œâ”€â”€ Cloud COLD (300GB S3-IA)   $40  (90-365 days, $0.0125/GB)
â”œâ”€â”€ Cloud ARCHIVE (1TB Glacier) $10  (>1 year, $0.004/GB)
â””â”€â”€ TOTAL:                     $245/month (1.2TB stored vs 225GB)

SCALABILITY (Peak Fire Season):
â”œâ”€â”€ PostgreSQL HOT (30GB)      $150 (2x load)
â”œâ”€â”€ Redis Cache (20GB)         $160 (2x cache)
â”œâ”€â”€ MinIO WARM (150GB)         $75  (More recent data)
â”œâ”€â”€ Cloud COLD (500GB)         $65  (Slower archival)
â”œâ”€â”€ Cloud ARCHIVE (1.5TB)      $15  (Glacier grows slowly)
â””â”€â”€ TOTAL:                     $465/month (2.2TB stored)

YEAR 3 PROJECTION (No Deletion, Just Archival):
â”œâ”€â”€ PostgreSQL (15GB)          $75  (Fixed size with lifecycle)
â”œâ”€â”€ Redis (10GB)               $80  (Fixed size, ephemeral)
â”œâ”€â”€ MinIO WARM (100GB)         $50  (90-day rolling window)
â”œâ”€â”€ Cloud COLD (1TB)           $130 (12 months of data)
â”œâ”€â”€ Cloud ARCHIVE (10TB)       $100 (3 years of archives)
â””â”€â”€ TOTAL:                     $435/month (11TB stored!)
```

**Key Insight:** With lifecycle policies, storage costs grow **sub-linearly** while data volume grows **linearly**.

---

### 4.B. Query Performance Optimization

```
BEFORE (All data in PostgreSQL):
â”œâ”€â”€ Query last 7 days of fires:     50ms  âœ…
â”œâ”€â”€ Query last 30 days of fires:    2s    âš ï¸
â”œâ”€â”€ Query last year of fires:       30s   âŒ
â”œâ”€â”€ Join fires + weather (30 days): 10s   âŒ

AFTER (Tiered + Indexed):
â”œâ”€â”€ Query last 7 days (PostgreSQL):       30ms  âœ…âœ…
â”œâ”€â”€ Query last 30 days (MinIO Parquet):   500ms âœ…
â”œâ”€â”€ Query last year (DuckDB on Parquet):  2s    âœ…
â”œâ”€â”€ Join fires + weather (Spark on S3):   5s    âœ…
```

**Optimization Techniques:**
1. **Partition Pruning**: Only scan relevant date partitions
2. **Columnar Reads**: Read only needed columns from Parquet
3. **Predicate Pushdown**: Filter at storage layer, not in memory
4. **Caching**: Redis for hot queries, CDN for static maps
5. **Pre-aggregation**: TimescaleDB continuous aggregates

---

## 5. Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2) âœ… PARTIALLY COMPLETE
- [x] Kafka cluster setup
- [x] PostgreSQL with TimescaleDB
- [x] MinIO object storage
- [ ] Airflow orchestration
- [ ] Metadata catalog tables

### Phase 2: Lifecycle Automation (Weeks 3-4)
- [ ] Implement Airflow DAGs for daily migrations
- [ ] Create Parquet export scripts (PostgreSQL â†’ MinIO)
- [ ] Add metadata tracking to data catalog
- [ ] Test hotâ†’warmâ†’cold transitions

### Phase 3: Cloud Integration (Weeks 5-6)
- [ ] Set up AWS S3 / Azure Blob for cold tier
- [ ] Configure Glacier/Archive for long-term storage
- [ ] Implement data replication policies
- [ ] Test failover and disaster recovery

### Phase 4: Optimization (Weeks 7-8)
- [ ] Tune Kafka retention and compression
- [ ] Optimize Parquet partitioning strategy
- [ ] Implement Delta Lake / Iceberg (optional)
- [ ] Set up monitoring dashboards (Grafana)

### Phase 5: Production Hardening (Weeks 9-10)
- [ ] Load testing (simulate fire season peak)
- [ ] Security audit (encryption, access controls)
- [ ] Documentation and runbooks
- [ ] Training for operations team

---

## 6. Technology Stack Recommendations

### On-Premises Stack (What You Have)
- âœ… **Kafka**: Event streaming
- âœ… **PostgreSQL + TimescaleDB**: Time-series DB
- âœ… **MinIO**: S3-compatible object storage
- âœ… **Redis**: Caching
- âœ… **Docker + Docker Compose**: Container orchestration

### Add for Production
- ğŸ†• **Apache Airflow**: Workflow orchestration (data lifecycle)
- ğŸ†• **DuckDB**: Fast analytics on Parquet (alternative to Spark for small-medium data)
- ğŸ†• **Delta Lake / Apache Iceberg**: ACID on object storage (optional, for advanced use cases)
- ğŸ†• **Prometheus + Grafana**: Already have this! Just add storage dashboards

### Cloud Services (For COLD/ARCHIVE Tiers)
- **AWS**: S3 Standard-IA (warm), S3 Glacier (cold), S3 Glacier Deep Archive (archive)
- **Azure**: Blob Cool Tier, Blob Archive Tier
- **GCP**: Cloud Storage Nearline, Coldline, Archive

---

## 7. Example Queries & Access Patterns

### Query 1: "Show all fires in the last 7 days"
```sql
-- Fast! Reads from PostgreSQL HOT tier
SELECT * FROM fire_detections
WHERE detection_time >= NOW() - INTERVAL '7 days'
AND confidence > 80;
```

### Query 2: "Show all fires in the last 3 months"
```python
# Reads from MinIO Parquet WARM tier
import duckdb
con = duckdb.connect()
result = con.execute("""
    SELECT * FROM read_parquet('s3://wildfire-data/fires/viirs/2025/*/*/*.parquet')
    WHERE detection_time >= current_date - INTERVAL '90 days'
    AND confidence > 80
""").fetchdf()
```

### Query 3: "Analyze fire trends from 2020-2024"
```python
# Reads from Cloud S3 COLD tier (slower but works!)
import pyarrow.parquet as pq
import s3fs
s3 = s3fs.S3FileSystem()
dataset = pq.ParquetDataset(
    's3://wildfire-archive/fires/',
    filesystem=s3,
    filters=[('year', '>=', 2020), ('year', '<=', 2024)]
)
df = dataset.read_pandas().to_pandas()
```

---

## 8. Monitoring & Alerting

### Key Metrics to Track
```yaml
Storage Metrics:
  - Total storage used (by tier)
  - Growth rate (GB/day)
  - Cost per GB (by tier)
  - Data age distribution

Performance Metrics:
  - Query latency (p50, p95, p99)
  - Throughput (queries/sec)
  - Cache hit rate (Redis)
  - Kafka consumer lag

Data Quality Metrics:
  - Ingestion success rate
  - Schema validation failures
  - Duplicate detection rate
  - Missing data gaps

Operational Metrics:
  - Airflow DAG success rate
  - Data migration job duration
  - Backup/restore test results
  - Storage tier transition errors
```

### Grafana Dashboard Panels
1. **Storage Capacity**: Current usage vs limits (by tier)
2. **Data Lifecycle**: Flow diagram showing data movement
3. **Cost Tracking**: Monthly spend by tier
4. **Query Performance**: Latency heatmap
5. **Kafka Health**: Consumer lag, partition distribution

---

## 9. Next Steps

1. **Review this architecture** with your team
2. **Set up Airflow** for orchestration (Docker Compose service)
3. **Implement first lifecycle DAG** (hot â†’ warm migration)
4. **Create metadata catalog schema** in PostgreSQL
5. **Test with 30 days of historical data**
6. **Gradually expand** to cloud tiers

This architecture will prevent the storage issues we just fixed and provide a scalable, maintainable foundation for your wildfire intelligence platform! ğŸ”¥

---

## Appendix: Code Examples

See separate files:
- `services/data-lifecycle-manager/` (new service)
- `airflow/dags/` (DAG examples)
- `docs/operations/data-retention-runbook.md`
